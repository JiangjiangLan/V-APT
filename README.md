# V-APT: Action Recognition Based on Image to Video Prompt-Tuning



# 1. Introduction
We propose the V-APT method, which incorporates a lightweight LSTM block for temporal relationship modeling before average temporal pooling. Furthermore, our approach also encompasses a deep visual and text prompt interaction block, which guides visual prompts through text prompts in order to ensure mutual coordination between vision and language.

# 2. Method
![image](https://github.com/JiangjiangLan/V-APT/assets/73203790/bd8267b8-8b97-4ccb-98fa-20a7e2e7fc9e)

# 3. Experiments

### Dataset

The specific experimental code and data will be released after the paper is accepted. Please stay tuned.







# Acknowledgment

This research is supported by the "Tianjin University of Technology Postgraduate Scientific Research Innovation Project"(No. YJ2390), thanks to the Tianjin University of Technology for its funding, and thanks to the TECHNICAL COLLEGE FOR THE DEAF for providing better resources.
